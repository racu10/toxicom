{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Clasificación por Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import ast\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "# Linear Models \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, LSTM, Input, RNN\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D \n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.utils import plot_model\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>listOfCleanWords</th>\n",
       "      <th>cleanWordsAsText</th>\n",
       "      <th>BagOfWords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['explanation', 'edit', 'make', 'username', 'h...</td>\n",
       "      <td>explanation edit make username hardcore metall...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['aww', 'match', 'background', 'colour', 'seem...</td>\n",
       "      <td>aww match background colour seemingly stick th...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['hey', 'man', 'really', 'try', 'edit', 'war',...</td>\n",
       "      <td>hey man really try edit war guy constantly rem...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['make', 'real', 'suggestions', 'improvement',...</td>\n",
       "      <td>make real suggestions improvement wonder secti...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['sir', 'hero', 'chance', 'remember', 'page']</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00025465d4725e87</td>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['congratulations', 'well', 'use', 'tool', 'we...</td>\n",
       "      <td>congratulations well use tool well talk</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['cocksucker', 'piss', 'around', 'work']</td>\n",
       "      <td>cocksucker piss around work</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00031b1e95af7921</td>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['vandalism', 'matt', 'shirvington', 'article'...</td>\n",
       "      <td>vandalism matt shirvington article revert plea...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00037261f536c51d</td>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['sorry', 'word', 'nonsense', 'offensive', 'an...</td>\n",
       "      <td>sorry word nonsense offensive anyway intend wr...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00040093b2687caa</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['alignment', 'subject', 'contrary', 'dulithgow']</td>\n",
       "      <td>alignment subject contrary dulithgow</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "5  00025465d4725e87  \"\\n\\nCongratulations from me as well, use the ...      0   \n",
       "6  0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1   \n",
       "7  00031b1e95af7921  Your vandalism to the Matt Shirvington article...      0   \n",
       "8  00037261f536c51d  Sorry if the word 'nonsense' was offensive to ...      0   \n",
       "9  00040093b2687caa  alignment on this subject and which are contra...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0             0        0       0       0              0   \n",
       "1             0        0       0       0              0   \n",
       "2             0        0       0       0              0   \n",
       "3             0        0       0       0              0   \n",
       "4             0        0       0       0              0   \n",
       "5             0        0       0       0              0   \n",
       "6             1        1       0       1              0   \n",
       "7             0        0       0       0              0   \n",
       "8             0        0       0       0              0   \n",
       "9             0        0       0       0              0   \n",
       "\n",
       "                                    listOfCleanWords  \\\n",
       "0  ['explanation', 'edit', 'make', 'username', 'h...   \n",
       "1  ['aww', 'match', 'background', 'colour', 'seem...   \n",
       "2  ['hey', 'man', 'really', 'try', 'edit', 'war',...   \n",
       "3  ['make', 'real', 'suggestions', 'improvement',...   \n",
       "4      ['sir', 'hero', 'chance', 'remember', 'page']   \n",
       "5  ['congratulations', 'well', 'use', 'tool', 'we...   \n",
       "6           ['cocksucker', 'piss', 'around', 'work']   \n",
       "7  ['vandalism', 'matt', 'shirvington', 'article'...   \n",
       "8  ['sorry', 'word', 'nonsense', 'offensive', 'an...   \n",
       "9  ['alignment', 'subject', 'contrary', 'dulithgow']   \n",
       "\n",
       "                                    cleanWordsAsText      BagOfWords  \n",
       "0  explanation edit make username hardcore metall...  <class 'dict'>  \n",
       "1  aww match background colour seemingly stick th...  <class 'dict'>  \n",
       "2  hey man really try edit war guy constantly rem...  <class 'dict'>  \n",
       "3  make real suggestions improvement wonder secti...  <class 'dict'>  \n",
       "4                      sir hero chance remember page  <class 'dict'>  \n",
       "5            congratulations well use tool well talk  <class 'dict'>  \n",
       "6                        cocksucker piss around work  <class 'dict'>  \n",
       "7  vandalism matt shirvington article revert plea...  <class 'dict'>  \n",
       "8  sorry word nonsense offensive anyway intend wr...  <class 'dict'>  \n",
       "9               alignment subject contrary dulithgow  <class 'dict'>  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nameTrainCSV = 'trainWithListOfCleanWords'\n",
    "nameTestCSV = 'testWithListOfCleanWords'\n",
    "\n",
    "train = pd.read_csv('../data/processed/' + nameTrainCSV + '.csv', encoding='utf-8')\n",
    "train['BagOfWords'] = dict\n",
    "train.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.124248027801514\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for x in tqdm(range(len(train))):\n",
    "    train.set_value(col='listOfCleanWords',\n",
    "                index=x,\n",
    "                value=ast.literal_eval(train[\"listOfCleanWords\"][x]))\n",
    "    train.set_value(col='cleanWordsAsText',\n",
    "                index=x,\n",
    "                value=str(train[\"cleanWordsAsText\"][x]))\n",
    "    train.set_value(col='BagOfWords',\n",
    "                index=x,\n",
    "                value=Counter(train[\"listOfCleanWords\"][x]))\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MULTICLASS PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classLabel = {\n",
    "    0: \"neutral\",\n",
    "    1: \"toxic\",\n",
    "    2 : \"severe_toxic\",\n",
    "    3 : \"obscene\",\n",
    "    4 : \"threat\",\n",
    "    5 : \"insult\",\n",
    "    6 : \"identity_hate\" \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.empty((len(train['cleanWordsAsText']),),dtype=object)\n",
    "allTextToxicTrain = dict()\n",
    "for idx in classLabel:\n",
    "    if classLabel[idx] != \"neutral\":\n",
    "        T = np.where(train[classLabel[idx]] == 1)[0]\n",
    "        allTextToxicTrain[idx] = T\n",
    "        for i in T:\n",
    "            if y[i] is None:\n",
    "                y[i] = [idx]                \n",
    "            else:\n",
    "                y[i].append(idx)\n",
    "indxsOfNeutralTexts = np.where(y == None) \n",
    "y[indxsOfNeutralTexts] = [[0]]\n",
    "indxsOfNeutralTexts = indxsOfNeutralTexts[0]\n",
    "\n",
    "allTextsNoToxicTrain = [str(train['cleanWordsAsText'][x]) for x in indxsOfNeutralTexts]\n",
    "\n",
    "idxList = []\n",
    "for i in allTextToxicTrain.keys():\n",
    "    #allTextToxicTrain[i] = [str(train['cleanWordsAsText'][j]) for j in allTextToxicTrain[i]]\n",
    "    idxList = np.unique(np.append(idxList, allTextToxicTrain[i]))\n",
    "allTextToxicTrain = [str(train['cleanWordsAsText'][j]) for j in idxList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test clasification\n",
    "test = pd.read_csv('../data/processed/' + nameTestCSV + '.csv', encoding='utf-8')\n",
    "test.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Se inicializan las variables de X_train y X_test + Y_train completos a partir de los textos ya limpios, ademas de obtener todos los textos en forma de lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allTrainText = [txt if txt is not np.nan else '' for txt in train['cleanWordsAsText']]\n",
    "allTestText = [txt if txt is not np.nan else '' for txt in test['cleanWordsAsText']]\n",
    "X_train = allTrainText\n",
    "X_test = allTestText\n",
    "yBinary = MultiLabelBinarizer().fit_transform(y)\n",
    "y_train = yBinary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = [\"idExp\",\"numFeatures\", \"algorithm\", \"Nfolds\", \"accuaracy\", \"logloss\", \"fmeasure\"]\n",
    "dfTestResults = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Cantidad features a utilizar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxFeatures = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedings\n",
    "> En este apartado se diseñan 3 formas de representación de las palabras de los textos, utilizando directamente los textos ya limpiados previamente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mediante vector TFID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vectorizer all text\n",
    "tfidV = TfidfVectorizer(ngram_range=(1,6), max_features=maxFeatures)\n",
    "X_train_tfid = tfidV.fit_transform(allTrainText)\n",
    "\n",
    "# Fit all clasificators with tfid matrix\n",
    "numFeatures = len(tfidV.get_feature_names())\n",
    "print(\"NFeatures = \" + str(numFeatures))\n",
    "tfidVTest = TfidfVectorizer(vocabulary=tfidV.get_feature_names())\n",
    "X_test_tfid = tfidVTest.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mediante BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BOW\n",
    "bowFeatures = CountVectorizer(vocabulary=tfidV.get_feature_names())\n",
    "X_train_bow = bowFeatures.fit_transform(allTrainText)\n",
    "\n",
    "X_test_bow = bowFeatures.fit_transform(allTestText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mediante Tokenización de las palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "\n",
    "tokenizer = Tokenizer(num_words=maxFeatures)\n",
    "tokenizer.fit_on_texts(list(allTrainText))\n",
    "X_train_tokenized_seq = tokenizer.texts_to_sequences(allTrainText)\n",
    "X_test_tokenized_seq = tokenizer.texts_to_sequences(allTestText)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_seq = pad_sequences(X_train_tokenized_seq)\n",
    "X_test_seq = pad_sequences(X_test_tokenized_seq, maxlen=len(X_train_seq[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación del modelo CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MODEL CNN\n",
    "numClases = 7\n",
    "\n",
    "#training params\n",
    "batch_size = 512 \n",
    "num_epochs = 8 \n",
    "\n",
    "#model parameters\n",
    "num_filters = 128 \n",
    "weight_decay = 1e-4\n",
    "outputDim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(X_train), output_dim=outputDim))\n",
    "model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(num_filters, 7, activation='relu', padding='same'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Dense(numClases, activation='sigmoid'))  #multi-label (k-hot encoding)\n",
    "\n",
    "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> En este caso hacemos fitting mediante Tokenización, debido a que para clasificar textos a partir de CNN es la que mejor resultados da. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model training\n",
    "cnnmModelHist = model.fit(X_train_seq, y_train, batch_size=batch_size, epochs=num_epochs, validation_split=0.1, shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = model.predict(X_test_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Se almacenan los datos predecidos en formato CSV para poder hacer el submision de test, y poder evaluar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = [\"id\",\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "dfTestPredicted = pd.DataFrame(columns=columns)\n",
    "for x in tqdm(range(len(test))):\n",
    "    dfTestPredicted.loc[x] = [test['id'][x], predicted[x][1], predicted[x][2], predicted[x][3], predicted[x][4], predicted[x][5], predicted[x][6]]\n",
    "dfTestPredicted.to_csv('../reports/testPred/predTestCNN_Seq_'+ str(maxFeatures) +'.csv',encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Creación del modelo LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(len(X_train_seq[0]), ))\n",
    "embed_size = 128\n",
    "x = Embedding(maxFeatures, embed_size)(inp)\n",
    "x = LSTM(90, return_sequences=True,name='lstm_layer')(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(60, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(7, activation=\"sigmoid\")(x)\n",
    "modelLSTM = Model(inputs=inp, outputs=x)\n",
    "modelLSTM.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso hacemos fitting mediante Tokenización, debido a que para clasificar textos a partir de LSTM es la que mejor resultados da."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelLSTM.fit(X_train_seq, y_train, batch_size=batch_size, epochs=min(num_epochs,2), validation_split=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Se almacenan los datos predecidos en formato CSV para poder hacer el submision de test, y poder evaluar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = modelLSTM.predict(X_test_seq, batch_size=1024, verbose=1)\n",
    "columns = [\"id\",\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "dfTestPredicted = pd.DataFrame(columns=columns)\n",
    "for x in tqdm(range(len(test))):\n",
    "    dfTestPredicted.loc[x] = [test['id'][x], predicted[x][1], predicted[x][2], predicted[x][3], predicted[x][4], predicted[x][5], predicted[x][6]]\n",
    "dfTestPredicted.to_csv('../reports/testPred/predTestLSTM_Seq_'+ str(maxFeatures) +'.csv',encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
